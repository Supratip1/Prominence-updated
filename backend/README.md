# Domain Content Scraper ğŸ•·ï¸

A powerful backend service that extracts text content and structure from domains using Scrapy and FastAPI.

**Perfect for hackathons!** ğŸš€ Built for rapid development and easy deployment.

## ğŸ¯ What It Does

- Takes a domain URL as input
- Crawls all discoverable pages on the domain
- Extracts structured text content (headings, paragraphs, links)
- Returns organized JSON with content hierarchy
- Respects robots.txt and implements rate limiting
- Tracks scraping progress with job status

## ğŸ—ï¸ Architecture

```
FastAPI Backend â†” Scrapy Engine â†” Target Websites
     â†“
SQLite Database (coming in Phase 4)
```

## ğŸš€ Quick Start

### 1. Setup Environment
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run the API Server
```bash
uvicorn app.main:app --reload
```

### 3. Test the API
Visit: http://localhost:8000/docs for interactive API documentation

Or test with curl:
```bash
curl -X POST "http://localhost:8000/scrape" \
     -H "Content-Type: application/json" \
     -d '{"domain": "example.com", "max_pages": 10}'
```

### 4. Test Scrapy Spider (Direct)
```bash
cd scrapy_project
scrapy crawl domain_spider -a domain=example.com -a max_pages=5
```

## ğŸ“Š API Endpoints

### `POST /scrape`
Start scraping a domain
```json
{
  "domain": "example.com",
  "max_pages": 50,
  "include_subdomains": false
}
```

### `GET /status/{job_id}`
Check scraping progress
```json
{
  "job_id": "uuid",
  "status": "running",
  "pages_scraped": 25,
  "progress": "50%"
}
```

### `GET /results/{job_id}`
Get extracted content
```json
{
  "content": [
    {
      "url": "https://example.com/page1",
      "title": "Page Title",
      "headings": ["H1 Text", "H2 Text"],
      "paragraphs": ["Paragraph content..."],
      "structure": {
        "h1": 1, "h2": 3, "p": 5, "a": 10
      }
    }
  ]
}
```

## ğŸ› ï¸ Development Status

### âœ… Phase 1: Complete (30 min)
- [x] Project structure created
- [x] Virtual environment setup
- [x] Dependencies installed
- [x] Basic FastAPI app
- [x] Scrapy project scaffolding
- [x] Pydantic models defined

### ğŸ”„ Phase 2: Next (45 min)
- [ ] Complete Scrapy spider implementation
- [ ] Content extraction rules
- [ ] Link following logic
- [ ] Error handling

### ğŸ”„ Phase 3: FastAPI Backend (30 min)
- [ ] Implement API endpoints
- [ ] Request/response validation
- [ ] Job management

### ğŸ”„ Phase 4: Data Storage (20 min)
- [ ] SQLite database setup
- [ ] Job persistence
- [ ] Content storage

### ğŸ”„ Phase 5: Integration (35 min)
- [ ] Async Scrapy execution
- [ ] Job queue management
- [ ] End-to-end testing

### ğŸ”„ Phase 6: MVP Features (30 min)
- [ ] Domain validation
- [ ] Rate limiting
- [ ] Content deduplication
- [ ] Progress tracking

## ğŸ® Demo Commands

```bash
# Health check
curl http://localhost:8000/health

# Start scraping
curl -X POST "http://localhost:8000/scrape" \
     -H "Content-Type: application/json" \
     -d '{"domain": "httpbin.org", "max_pages": 5}'

# Check status
curl http://localhost:8000/status/your-job-id

# Get results
curl http://localhost:8000/results/your-job-id
```

## ğŸ”§ Configuration

### Scrapy Settings (`scrapy_project/settings.py`)
- Respects robots.txt
- 1-second delay between requests
- Max 500 pages per domain
- Auto-throttling enabled
- Content validation pipelines

### FastAPI Settings (`app/main.py`)
- Async request handling
- Pydantic validation
- Interactive docs at `/docs`
- CORS enabled for development

## ğŸ“¦ Project Structure

```
domain-scraper/
â”œâ”€â”€ app/                     # FastAPI application
â”‚   â”œâ”€â”€ main.py             # API endpoints
â”‚   â”œâ”€â”€ models.py           # Pydantic models
â”‚   â”œâ”€â”€ database.py         # SQLite operations
â”‚   â””â”€â”€ scraper.py          # Scrapy integration
â”œâ”€â”€ scrapy_project/         # Scrapy project
â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â””â”€â”€ domain_spider.py # Main spider
â”‚   â”œâ”€â”€ items.py            # Data structures
â”‚   â”œâ”€â”€ pipelines.py        # Data processing
â”‚   â””â”€â”€ settings.py         # Configuration
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ README.md              # This file
```

## ğŸš§ MVP Limitations (Hackathon-Friendly)

- Single-threaded scraping
- In-memory job storage (Phase 1-3)
- Basic error handling
- Limited to text content
- No authentication
- Simple rate limiting

## ğŸš€ Future Enhancements

- [ ] Multi-threaded scraping
- [ ] Redis/PostgreSQL storage
- [ ] Image/video asset extraction
- [ ] AI-powered content analysis
- [ ] User authentication
- [ ] Docker containerization
- [ ] Webhook notifications
- [ ] Advanced filtering options

## ğŸ“ Notes for Hackathon

- **Total MVP Time**: ~4.5 hours
- **Perfect for demos**: Visual progress tracking
- **Easy to extend**: Modular architecture
- **Well-documented**: Clear API and code structure
- **Respectful scraping**: Follows best practices

## ğŸ† Ready to Scale

This MVP can handle:
- Small to medium websites (up to 500 pages)
- Multiple concurrent domains
- Structured data extraction
- Real-time progress tracking

Perfect foundation for building more advanced features post-hackathon!

---

**Happy Hacking!** ğŸ¯ Built with â¤ï¸ for rapid prototyping